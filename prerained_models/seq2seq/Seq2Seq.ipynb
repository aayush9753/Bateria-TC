{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/attention-seq2seq-with-pytorch-learning-to-invert-a-sequence-34faf4133e53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'acbbdcc'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = list(\"abcd\")\n",
    "''.join([choice(t[:]) for _ in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from random import choice, randrange\n",
    "class ToyDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Inspired from https://talbaumel.github.io/blog/attention/\n",
    "    \"\"\"\n",
    "    def __init__(self, min_length=5, max_length=20, type='train'):\n",
    "        self.SOS = \"<s>\"  \n",
    "        self.EOS = \"</s>\" \n",
    "        self.characters = list(\"abcd\")\n",
    "        self.int2char = list(self.characters)\n",
    "        self.char2int = {c: i+3 for i, c in enumerate(self.characters)}\n",
    "        self.VOCAB_SIZE = len(self.characters)\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        if type=='train':\n",
    "            self.set = [self._sample() for _ in range(3000)]\n",
    "        else:\n",
    "            self.set = [self._sample() for _ in range(300)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.set)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.set[item]\n",
    "\n",
    "    def _sample(self):\n",
    "        random_length = randrange(self.min_length, self.max_length)# Pick a random length\n",
    "        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n",
    "        random_string = ''.join(random_char_list)\n",
    "        a = np.array([self.char2int.get(x) for x in random_string])\n",
    "        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse\n",
    "        x = np.zeros((random_length, self.VOCAB_SIZE))\n",
    "        x[np.arange(random_length), a-3] = 1\n",
    "        return x, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.]]),\n",
       " array([3, 5, 4, 4, 3, 3, 4, 3, 5, 3, 3, 5, 2]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToyDataset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ToyDataset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = config[\"n_channels\"]\n",
    "        self.hidden_size = config[\"encoder_hidden\"]\n",
    "        self.layers = config.get(\"encoder_layers\", 1)\n",
    "        self.dnn_layers = config.get(\"encoder_dnn_layers\", 0)\n",
    "        self.dropout = config.get(\"encoder_dropout\", 0.)\n",
    "        self.bi = config.get(\"bidirectional_encoder\", False)\n",
    "        if self.dnn_layers > 0:\n",
    "            for i in range(self.dnn_layers):\n",
    "                self.add_module('dnn_' + str(i), nn.Linear(\n",
    "                    in_features=self.input_size if i == 0 else self.hidden_size,\n",
    "                    out_features=self.hidden_size\n",
    "                ))\n",
    "        gru_input_dim = self.input_size if self.dnn_layers == 0 else self.hidden_size\n",
    "        self.rnn = nn.GRU(\n",
    "            gru_input_dim,\n",
    "            self.hidden_size,\n",
    "            self.layers,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=self.bi,\n",
    "            batch_first=True)\n",
    "        self.gpu = config.get(\"gpu\", False)\n",
    "\n",
    "    def run_dnn(self, x):\n",
    "        for i in range(self.dnn_layers):\n",
    "            x = F.relu(getattr(self, 'dnn_'+str(i))(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs, hidden, input_lengths):\n",
    "        if self.dnn_layers > 0:\n",
    "            inputs = self.run_dnn(inputs)\n",
    "        x = pack_padded_sequence(inputs, input_lengths, batch_first=True)\n",
    "        output, state = self.rnn(x, hidden)\n",
    "        output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.)\n",
    "\n",
    "        if self.bi:\n",
    "            output = output[:, :, :self.hidden_size] + output[:, :, self.hidden_size:]\n",
    "        return output, state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = Variable(torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size))\n",
    "        if self.gpu:\n",
    "            h0 = h0.cuda()\n",
    "        return h0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.hidden_size = config[\"decoder_hidden\"]\n",
    "        embedding_dim = config.get(\"embedding_dim\", None)\n",
    "        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n",
    "        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.embedding_dim+self.hidden_size if config['decoder'].lower() == 'bahdanau' else self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=config.get(\"decoder_layers\", 1),\n",
    "            dropout=config.get(\"decoder_dropout\", 0),\n",
    "            bidirectional=config.get(\"bidirectional_decoder\", False),\n",
    "            batch_first=True)\n",
    "        if config['decoder'] != \"RNN\":\n",
    "            self.attention = Attention(\n",
    "                self.batch_size,\n",
    "                self.hidden_size,\n",
    "                method=config.get(\"attention_score\", \"dot\"),\n",
    "                mlp=config.get(\"attention_mlp_pre\", False))\n",
    "\n",
    "        self.gpu = config.get(\"gpu\", False)\n",
    "        self.decoder_output_fn = F.log_softmax if config.get('loss', 'NLL') == 'NLL' else None\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\" Must be overrided \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(Decoder):\n",
    "    def __init__(self, config):\n",
    "        super(RNNDecoder, self).__init__(config)\n",
    "        self.output_size = config.get(\"n_classes\", 32)\n",
    "        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        input = kwargs[\"input\"]\n",
    "        hidden = kwargs[\"hidden\"]\n",
    "\n",
    "        embedded = self.embedding(input).unsqueeze(0)\n",
    "        rnn_input = torch.cat((embedded, hidden.unsqueeze(0)), 2)\n",
    "        rnn_output, rnn_hidden = self.rnn(embedded.transpose(1, 0), hidden.unsqueeze(0))\n",
    "        output = rnn_output.squeeze(1)\n",
    "        output = self.character_distribution(output)\n",
    "\n",
    "        if self.decoder_output_fn:\n",
    "            output = self.decoder_output_fn(output, -1)\n",
    "\n",
    "        return output, rnn_hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(Decoder):\n",
    "    \"\"\"\n",
    "        Corresponds to AttnDecoderRNN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(AttentionDecoder, self).__init__(config)\n",
    "        self.output_size = config.get(\"n_classes\", 32)\n",
    "        self.character_distribution = nn.Linear(2*self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param input: [B]\n",
    "        :param prev_context: [B, H]\n",
    "        :param prev_hidden: [B, H]\n",
    "        :param encoder_outputs: [B, T, H]\n",
    "        :return: output (B, V), context (B, H), prev_hidden (B, H), weights (B, T)\n",
    "        Official Tensorflow documentation says : Perform a step of attention-wrapped RNN.\n",
    "        - Step 1: Mix the `inputs` and previous step's `attention` output via\n",
    "          `cell_input_fn`.\n",
    "        - Step 2: Call the wrapped `cell` with this input and its previous state.\n",
    "        - Step 3: Score the cell's output with `attention_mechanism`.\n",
    "        - Step 4: Calculate the alignments by passing the score through the\n",
    "          `normalizer`.\n",
    "        - Step 5: Calculate the context vector as the inner product between the\n",
    "          alignments and the attention_mechanism's values (memory).\n",
    "        - Step 6: Calculate the attention output by concatenating the cell output\n",
    "          and context through the attention layer (a linear layer with\n",
    "          `attention_layer_size` outputs).\n",
    "        Args:\n",
    "          inputs: (Possibly nested tuple of) Tensor, the input at this time step.\n",
    "          state: An instance of `AttentionWrapperState` containing\n",
    "            tensors from the previous time step.\n",
    "        Returns:\n",
    "          A tuple `(attention_or_cell_output, next_state)`, where:\n",
    "          - `attention_or_cell_output` depending on `output_attention`.\n",
    "          - `next_state` is an instance of `AttentionWrapperState`\n",
    "             containing the state calculated at this time step.\n",
    "        Raises:\n",
    "          TypeError: If `state` is not an instance of `AttentionWrapperState`.\n",
    "        \"\"\"\n",
    "        input = kwargs[\"input\"]\n",
    "        prev_hidden = kwargs[\"prev_hidden\"]\n",
    "        encoder_outputs = kwargs[\"encoder_outputs\"]\n",
    "        seq_len = kwargs.get(\"seq_len\", None)\n",
    "\n",
    "        # RNN (Eq 7 paper)\n",
    "        embedded = self.embedding(input).unsqueeze(1) # [B, H]\n",
    "        prev_hidden = prev_hidden.unsqueeze(0)\n",
    "        rnn_output, hidden = self.rnn(embedded, prev_hidden)\n",
    "        rnn_output = rnn_output.squeeze(1)\n",
    "\n",
    "        # Attention weights (Eq 6 paper)\n",
    "        weights = self.attention.forward(rnn_output, encoder_outputs, seq_len) # B x T\n",
    "        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)  # [B x N]\n",
    "\n",
    "        # Projection (Eq 8 paper)\n",
    "        # /!\\ Don't apply tanh on outputs, it screws everything up\n",
    "        output = self.character_distribution(torch.cat((rnn_output, context), 1))\n",
    "\n",
    "        # Apply log softmax if loss is NLL\n",
    "        if self.decoder_output_fn:\n",
    "            output = self.decoder_output_fn(output, -1)\n",
    "\n",
    "        if len(output.size()) == 3:\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "        return output, hidden.squeeze(0), weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        last_hidden: (batch_size, hidden_size)\n",
    "        encoder_outputs: (batch_size, max_time, hidden_size)\n",
    "    Returns:\n",
    "        attention_weights: (batch_size, max_time)\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, hidden_size, method=\"dot\", mlp=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        if method == 'dot':\n",
    "            pass\n",
    "        elif method == 'general':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        elif method == \"concat\":\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        elif method == 'bahdanau':\n",
    "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.va = nn.Parameter(torch.FloatTensor(batch_size, hidden_size))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.mlp = mlp\n",
    "        if mlp:\n",
    "            self.phi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.psi = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n",
    "        batch_size, seq_lens, _ = encoder_outputs.size()\n",
    "        if self.mlp:\n",
    "            last_hidden = self.phi(last_hidden)\n",
    "            encoder_outputs = self.psi(encoder_outputs)\n",
    "\n",
    "        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n",
    "\n",
    "        if seq_len is not None:\n",
    "            attention_energies = mask_3d(attention_energies, seq_len, -float('inf'))\n",
    "\n",
    "        return F.softmax(attention_energies, -1)\n",
    "\n",
    "    def _score(self, last_hidden, encoder_outputs, method):\n",
    "        \"\"\"\n",
    "        Computes an attention score\n",
    "        :param last_hidden: (batch_size, hidden_dim)\n",
    "        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n",
    "        :param method: str (`dot`, `general`, `concat`, `bahdanau`)\n",
    "        :return: a score (batch_size, max_time)\n",
    "        \"\"\"\n",
    "\n",
    "        assert encoder_outputs.size()[-1] == self.hidden_size\n",
    "\n",
    "        if method == 'dot':\n",
    "            last_hidden = last_hidden.unsqueeze(-1)\n",
    "            return encoder_outputs.bmm(last_hidden).squeeze(-1)\n",
    "\n",
    "        elif method == 'general':\n",
    "            x = self.Wa(last_hidden)\n",
    "            x = x.unsqueeze(-1)\n",
    "            return encoder_outputs.bmm(x).squeeze(-1)\n",
    "\n",
    "        elif method == \"concat\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            x = F.tanh(self.Wa(torch.cat((x, encoder_outputs), 1)))\n",
    "            return x.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        elif method == \"bahdanau\":\n",
    "            x = last_hidden.unsqueeze(1)\n",
    "            out = F.tanh(self.Wa(x) + self.Ua(encoder_outputs))\n",
    "            return out.bmm(self.va.unsqueeze(2)).squeeze(-1)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'score={}, mlp_preprocessing={}'.format(\n",
    "            self.method, self.mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
